{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_CMI(yyyymmddhhmn, band, path_local, path_dest, product_name):\n",
    "    os.makedirs(path_local, exist_ok=True)\n",
    "    os.makedirs(path_dest, exist_ok=True)\n",
    "\n",
    "    year = datetime.datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%Y')\n",
    "    day_of_year = datetime.datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%j')\n",
    "    hour = datetime.datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%H')\n",
    "    min = datetime.datetime.strptime(yyyymmddhhmn, '%Y%m%d%H%M').strftime('%M')\n",
    "\n",
    "    # AMAZON repository information\n",
    "    bucket_name = 'noaa-goes16'\n",
    "\n",
    "    # Initializes the S3 client\n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "    # File structure\n",
    "    prefix = f'{product_name}/{year}/{day_of_year}/{hour}/OR_{product_name}-M6C{int(band):02.0f}_G16_s{year}{day_of_year}{hour}{min}'\n",
    "\n",
    "    # Search for the file on the server\n",
    "    s3_result = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter=\"/\")\n",
    "\n",
    "    # Check if there are files available\n",
    "    if 'Contents' not in s3_result:\n",
    "        # There are no files\n",
    "        print(f'No files found for the date: {yyyymmddhhmn}, Band-{band}')\n",
    "        return -1\n",
    "    else:\n",
    "        # There are files\n",
    "        for obj in s3_result['Contents']:\n",
    "            key = obj['Key']\n",
    "            # Extract the file name\n",
    "            file_name = key.split('/')[-1].split('.')[0]\n",
    "\n",
    "            # Check if the file exists in either path_local or path_dest\n",
    "            local_file_path = f'{path_local}/{file_name}.nc'\n",
    "            dest_file_path = f'{path_dest}/{file_name}.nc'\n",
    "\n",
    "            if os.path.exists(local_file_path):\n",
    "                print(f'File already exists in {path_local}: {local_file_path}')\n",
    "                return file_name\n",
    "            elif os.path.exists(dest_file_path):\n",
    "                print(f'File already exists in {path_dest}: {dest_file_path}')\n",
    "                return file_name\n",
    "            else:\n",
    "                # File doesn't exist in either location, proceed with download\n",
    "                print(f'Downloading file to {path_local}: {local_file_path}')\n",
    "                s3_client.download_file(bucket_name, key, local_file_path)\n",
    "                return file_name\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from dask import delayed\n",
    "\n",
    "def process_and_downscale_cmi(path_local, path_dest, date_save, time_save, product_name, Band):\n",
    "    # Create the necessary yyyymmddhhmn format for downloading\n",
    "    yyyymmddhhmn = date_save + time_save\n",
    "    \n",
    "    file_name = download_CMI(yyyymmddhhmn, Band, path_local, path_dest, product_name)\n",
    "    \n",
    "    # Check if the file already exists in the destination path\n",
    "    output_path = f'{path_dest}/{file_name}.nc'\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File {output_path} already exists. Skipping download and processing.\")\n",
    "        return\n",
    "    \n",
    "    # Construct the path for the downloaded file\n",
    "    ncs = [f'{path_local}{i}' for i in os.listdir(path_local) if i.endswith(file_name + '.nc')]\n",
    "    \n",
    "    if not ncs:\n",
    "        print(f\"No file found for {file_name}.\")\n",
    "        return\n",
    "\n",
    "    with xr.open_dataset(ncs[0]) as ds:\n",
    "        target_x_res = 300\n",
    "        target_y_res = 300\n",
    "        # Define geographic limits for cropping\n",
    "        ds_sel = delayed(ds.sel)(x=slice(-0.05, 0.07), y=slice(0.09, -0.03))\n",
    "        original_x_size = delayed(ds_sel.sizes['x'])\n",
    "        original_y_size = delayed(ds_sel.sizes['y'])\n",
    "        scale_factor_x = delayed(lambda ox: max(ox // target_x_res, 1))(original_x_size)\n",
    "        scale_factor_y = delayed(lambda oy: max(oy // target_y_res, 1))(original_y_size)\n",
    "\n",
    "        # Apply the downscaling factor to each image\n",
    "        ds_downscaled = delayed(ds_sel.coarsen)(x=scale_factor_x, y=scale_factor_y, boundary=\"trim\").mean()\n",
    "        downscaled = ds_downscaled.compute()\n",
    "\n",
    "        # Save the downscaled dataset\n",
    "        downscaled.to_netcdf(output_path)\n",
    "        print(f\"Downscaled file saved at {output_path}\")\n",
    "\n",
    "    # After the dataset is closed, you can safely delete the original file\n",
    "    temp_path = f'{path_local}/{file_name}.nc'\n",
    "    if os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "        print(f\"File {temp_path} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"File {temp_path} does not exist.\")\n",
    "\n",
    "\n",
    "# path_local = 'D:/RODRIGO/IntradayForecasting/content/GOES_Files/CMI/' # Ruta para archivos temporales\n",
    "# path_dest = 'D:/RODRIGO/IntradayForecasting/content/GOES_Files/CMIPF_sliced_resized/'\n",
    "# date_save = '20240705'\n",
    "# time_save = '1500'\n",
    "# product_name = 'ABI-L2-CMIPF'\n",
    "# Band = 5\n",
    "\n",
    "# process_and_downscale_cmi(path_local, path_dest, date_save, time_save, product_name, Band)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Function to generate time strings in hourly increments \n",
    "def generate_time_strings_daytime():\n",
    "    times = []\n",
    "    for hour in range(9, 24):  \n",
    "        times.append(f'{hour:02}00')\n",
    "    times.append('0000')  # (24:00 == 00:00)\n",
    "    return times\n",
    "\n",
    "def generate_time_strings():\n",
    "    times = []\n",
    "    for hour in range(0, 24):  # Generate times from 0000 to 2300\n",
    "        times.append(f'{hour:02}00')\n",
    "    return times\n",
    "\n",
    "# Function to process and downscale GOES data between two dates, at specified times\n",
    "def process_goes_between_dates(path_local, path_dest, start_date, end_date, product_name, band):\n",
    "    # Convert string dates to datetime objects\n",
    "    start_date_obj = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "    end_date_obj = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "\n",
    "    # Generate a list of hourly time strings\n",
    "    # times = generate_time_strings()\n",
    "    times = generate_time_strings_daytime()\n",
    "\n",
    "    # Iterate over each date between the start and end dates (inclusive)\n",
    "    current_date_obj = start_date_obj\n",
    "    while current_date_obj <= end_date_obj:\n",
    "        # Format the date as a string in 'YYYYMMDD' format\n",
    "        date_save = current_date_obj.strftime('%Y%m%d')\n",
    "\n",
    "        # Iterate over each time (0900, 1000, 1100, ..., 0000)\n",
    "        for time_save in times:\n",
    "            # process, and downscale the file\n",
    "            try:\n",
    "                print(f\"Processing for {date_save} at {time_save}\")\n",
    "                process_and_downscale_cmi(path_local, path_dest, date_save, time_save, product_name, band)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the process\n",
    "                print(f\"Error processing for {date_save} at {time_save}: {e}\")\n",
    "        \n",
    "        # Move to the next day\n",
    "        current_date_obj += datetime.timedelta(days=1)\n",
    "\n",
    "# Usage\n",
    "path_local = 'D:/RODRIGO/IntradayForecasting/content/GOES_Files/CMI/' # Ruta para archivos temporales\n",
    "path_dest = 'D:/RODRIGO/IntradayForecasting/content/GOES_Files/CMIPF_sliced_resized/'\n",
    "start_date = '20240801'  # Example start date\n",
    "end_date = '20240830'    # Example end date\n",
    "product_name = 'ABI-L2-CMIPF'\n",
    "band = 2 \n",
    "\n",
    "process_goes_between_dates(path_local, path_dest, start_date, end_date, product_name, band)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
